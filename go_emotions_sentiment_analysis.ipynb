{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoEmotions: 다중 감정 분류 모델 학습\n",
    "\n",
    "이 노트북은 GoEmotions 데이터셋을 사용하여 Reddit 댓글의 다중 감정(27가지)을 분류하는 모델을 학습하고 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers[torch] scikit-learn pandas numpy matplotlib seaborn tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, classification_report, multilabel_confusion_matrix\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub 로그인 (필요시)\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GoEmotions 데이터셋 로드\n",
    "dataset = load_dataset(\"go_emotions\", \"raw\") # 'raw' configuration for original multi-label data\n",
    "\n",
    "print(dataset)\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 정보 확인\n",
    "labels_list = dataset[\"train\"].features[\"labels\"].feature.names\n",
    "id2label = {idx: label for idx, label in enumerate(labels_list)}\n",
    "label2id = {label: idx for idx, label in enumerate(labels_list)}\n",
    "num_labels = len(labels_list)\n",
    "\n",
    "print(f\"Total labels: {num_labels}\")\n",
    "print(f\"Label names: {labels_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. 데이터셋 레이블 분포 시각화 (추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체 데이터셋 (train, validation, test 합쳐서) 레이블 분포 확인\n",
    "all_labels_flat = []\n",
    "for split in dataset.keys():\n",
    "    for example_labels in dataset[split]['labels']:\n",
    "        all_labels_flat.extend(example_labels)\n",
    "\n",
    "label_counts = Counter(all_labels_flat)\n",
    "df_label_counts = pd.DataFrame([(id2label[id], count) for id, count in label_counts.items()], columns=['Emotion', 'Count']).sort_values(by='Count', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Count', y='Emotion', data=df_label_counts, palette='viridis')\n",
    "plt.title('GoEmotions Dataset: Label Distribution (All Splits)')\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Emotion')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 데이터 전처리 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples):\n",
    "    # 텍스트 토큰화\n",
    "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    # 레이블을 multi-hot encoding으로 변환\n",
    "    multi_hot_labels = []\n",
    "    for label_list_item in examples[\"labels\"]:\n",
    "        one_hot_vector = [0.0] * num_labels # float 타입으로 초기화\n",
    "        for label_id in label_list_item:\n",
    "            if 0 <= label_id < num_labels:\n",
    "                one_hot_vector[label_id] = 1.0\n",
    "        multi_hot_labels.append(one_hot_vector)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = multi_hot_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_data, batched=True, remove_columns=[\"text\", \"id\", \"caller_id\", \"linked_id\", \"created_utc\", \"comment_id\"])\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "print(tokenized_datasets['train'][0])\n",
    "print(tokenized_datasets['train'][0]['labels'].shape)\n",
    "print(tokenized_datasets['train'][0]['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 크기 줄이기 (빠른 테스트용, 필요시 주석 해제)\n",
    "# print(\"Original dataset sizes:\")\n",
    "# print({split: len(tokenized_datasets[split]) for split in tokenized_datasets})\n",
    "\n",
    "# train_subset_size = 1000 \n",
    "# val_subset_size = 200\n",
    "# test_subset_size = 200\n",
    "\n",
    "# tokenized_datasets_subset = DatasetDict({\n",
    "#     'train': tokenized_datasets['train'].shuffle(seed=42).select(range(train_subset_size)),\n",
    "#     'validation': tokenized_datasets['validation'].shuffle(seed=42).select(range(val_subset_size)),\n",
    "#     'test': tokenized_datasets['test'].shuffle(seed=42).select(range(test_subset_size))\n",
    "# })\n",
    "# print(\"\\nSubset dataset sizes:\")\n",
    "# print({split: len(tokenized_datasets_subset[split]) for split in tokenized_datasets_subset})\n",
    "# tokenized_datasets = tokenized_datasets_subset # 실제 학습에 사용할 데이터셋을 부분집합으로 교체"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 선택 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_labels, \n",
    "    problem_type=\"multi_label_classification\", # 다중 레이블 분류 문제 명시\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 모델 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 학습 설정 및 메트릭 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다중 레이블 분류를 위한 메트릭 함수\n",
    "def compute_metrics_multilabel(eval_pred):\n",
    "    logits, true_labels = eval_pred\n",
    "    # 로짓에 시그모이드 함수 적용\n",
    "    probs = torch.sigmoid(torch.Tensor(logits)).numpy()\n",
    "    # 임계값(0.5)을 기준으로 예측 레이블 결정 (0 또는 1)\n",
    "    predictions = (probs > 0.5).astype(int)\n",
    "    \n",
    "    true_labels = true_labels.astype(int)\n",
    "\n",
    "    f1_macro = f1_score(true_labels, predictions, average='macro', zero_division=0)\n",
    "    f1_micro = f1_score(true_labels, predictions, average='micro', zero_division=0)\n",
    "    f1_weighted = f1_score(true_labels, predictions, average='weighted', zero_division=0)\n",
    "    subset_accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'f1_macro': f1_macro,\n",
    "        'f1_micro': f1_micro,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'subset_accuracy': subset_accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16 \n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3 \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_goemotions\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_dir=\"./logs_goemotions\",\n",
    "    logging_strategy=\"epoch\", # 로깅 전략을 epoch으로 변경\n",
    "    load_best_model_at_end=True, \n",
    "    metric_for_best_model=\"f1_macro\", \n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2, \n",
    "    fp16=torch.cuda.is_available(), \n",
    "    report_to=\"tensorboard\", \n",
    "    push_to_hub=False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Trainer 정의 및 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_multilabel,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard 실행 (Colab 또는 로컬 터미널에서)\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs_goemotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 학습 과정 시각화 (추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_history = trainer.state.log_history\n",
    "\n",
    "df_log = pd.DataFrame(log_history)\n",
    "\n",
    "# 학습 손실과 검증 손실 분리\n",
    "train_loss = df_log[df_log['loss'].notna()][['epoch', 'loss']]\n",
    "eval_loss = df_log[df_log['eval_loss'].notna()][['epoch', 'eval_loss']]\n",
    "eval_metrics = df_log[df_log['eval_f1_macro'].notna()]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 손실 시각화\n",
    "plt.subplot(1, 2, 1)\n",
    "if not train_loss.empty:\n",
    "    plt.plot(train_loss['epoch'], train_loss['loss'], label='Training Loss', marker='o')\n",
    "if not eval_loss.empty:\n",
    "    plt.plot(eval_loss['epoch'], eval_loss['eval_loss'], label='Validation Loss', marker='o')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# F1 Macro 시각화 (또는 다른 주요 평가지표)\n",
    "plt.subplot(1, 2, 2)\n",
    "if not eval_metrics.empty:\n",
    "    plt.plot(eval_metrics['epoch'], eval_metrics['eval_f1_macro'], label='Validation F1 Macro', marker='o', color='green')\n",
    "    # 다른 평가지표 추가 가능 (예: eval_subset_accuracy)\n",
    "    if 'eval_subset_accuracy' in eval_metrics.columns:\n",
    "      plt.plot(eval_metrics['epoch'], eval_metrics['eval_subset_accuracy'], label='Validation Subset Accuracy', marker='x', color='purple', linestyle='--')\n",
    "plt.title('Validation Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 평가 및 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋으로 평가\n",
    "eval_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"\\nTest Set Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋으로 예측\n",
    "predictions_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "logits = predictions_output.predictions\n",
    "true_labels_test = predictions_output.label_ids # 변수명 변경 true_labels -> true_labels_test\n",
    "\n",
    "probs = torch.sigmoid(torch.Tensor(logits)).numpy()\n",
    "predicted_labels_test = (probs > 0.5).astype(int) # 변수명 변경 predicted_labels -> predicted_labels_test\n",
    "\n",
    "true_labels_test = true_labels_test.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 분류 보고서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGoEmotions Multi-label Classification Report (Test Set):\")\n",
    "print(classification_report(true_labels_test, predicted_labels_test, target_names=labels_list, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 혼동 행렬 (각 레이블별)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 레이블에 대한 혼동 행렬 시각화\n",
    "mcm = multilabel_confusion_matrix(true_labels_test, predicted_labels_test)\n",
    "\n",
    "def plot_confusion_matrix_multilabel(mcm_data, current_labels_list, cols=3):\n",
    "    rows = (len(current_labels_list) + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 5))\n",
    "    axes = axes.flatten() \n",
    "    for i, label_name in enumerate(current_labels_list):\n",
    "        if i < len(mcm_data):\n",
    "            cm = mcm_data[i]\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
    "                        xticklabels=['Not ' + label_name, label_name],\n",
    "                        yticklabels=['Not ' + label_name, label_name])\n",
    "            axes[i].set_title(f'CM for: {label_name}')\n",
    "            axes[i].set_xlabel('Predicted')\n",
    "            axes[i].set_ylabel('True')\n",
    "        else:\n",
    "            axes[i].axis('off') \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix_multilabel(mcm, labels_list, cols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 샘플 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(text, model_to_predict, tokenizer_to_use, current_id2label, threshold=0.5):\n",
    "    model_to_predict.eval()\n",
    "    current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_to_predict.to(current_device)\n",
    "\n",
    "    inputs = tokenizer_to_use(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(current_device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_output = model_to_predict(**inputs).logits\n",
    "    \n",
    "    probs_output = torch.sigmoid(logits_output).squeeze().cpu().numpy()\n",
    "    \n",
    "    predicted_label_ids = np.where(probs_output > threshold)[0]\n",
    "    predicted_emotions_list = [current_id2label[idx] for idx in predicted_label_ids]\n",
    "    \n",
    "    return predicted_emotions_list, probs_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"I am so happy and excited about the new project!\",\n",
    "    \"This is really frustrating and annoying.\",\n",
    "    \"I'm not sure how I feel about this, a bit confused and curious.\",\n",
    "    \"Thank you so much, I really appreciate your help.\",\n",
    "    \"The movie was incredibly sad, but also very beautiful.\"\n",
    "]\n",
    "\n",
    "for text_item in sample_texts:\n",
    "    predicted_emotions, _ = predict_emotion(text_item, model, tokenizer, id2label, threshold=0.3) \n",
    "    print(f\"\\nSample Text: '{text_item}'\")\n",
    "    if predicted_emotions:\n",
    "        print(f\"Predicted Emotions: {', '.join(predicted_emotions)}\")\n",
    "    else:\n",
    "        print(\"Predicted Emotions: No emotion above threshold (or 'neutral' if available and predicted)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 저장 (선택 사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습된 모델과 토크나이저 저장\n",
    "output_model_dir = \"./saved_model_goemotions\"\n",
    "os.makedirs(output_model_dir, exist_ok=True)\n",
    "\n",
    "if hasattr(trainer, 'model'): # trainer가 모델을 가지고 있는지 확인\n",
    "    trainer.save_model(output_model_dir)\n",
    "    tokenizer.save_pretrained(output_model_dir)\n",
    "    print(f\"Model and tokenizer saved to {output_model_dir}\")\n",
    "else:\n",
    "    print(\"Trainer does not have a model to save. Was training completed?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드 및 사용 예시 (저장된 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(output_model_dir) and os.path.exists(os.path.join(output_model_dir, 'pytorch_model.bin')):\n",
    "    loaded_model = AutoModelForSequenceClassification.from_pretrained(output_model_dir)\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(output_model_dir)\n",
    "\n",
    "    sample_text_for_loading_test = \"I feel a bit nervous but also excited.\"\n",
    "    predicted_emotions_loaded, _ = predict_emotion(sample_text_for_loading_test, loaded_model, loaded_tokenizer, id2label, threshold=0.3)\n",
    "\n",
    "    print(f\"\\nSample Text (loaded model): '{sample_text_for_loading_test}'\")\n",
    "    if predicted_emotions_loaded:\n",
    "        print(f\"Predicted Emotions: {', '.join(predicted_emotions_loaded)}\")\n",
    "    else:\n",
    "        print(\"Predicted Emotions: No emotion above threshold\")\n",
    "else:\n",
    "    print(f\"Skipping loading example: Model not found at {output_model_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
