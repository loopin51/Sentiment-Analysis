{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial PhraseBank: 금융 감성 분석 모델 학습\n",
    "\n",
    "이 노트북은 Financial PhraseBank 데이터셋을 사용하여 금융 뉴스 헤드라인의 감성(긍정/중립/부정)을 분류하는 모델을 학습하고 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers[torch] scikit-learn pandas numpy matplotlib seaborn tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub 로그인 (필요시)\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 준비\n",
    "\n",
    "Financial PhraseBank 데이터셋은 일반적으로 `.txt` 파일 형태로 제공되며, 각 라인은 `텍스트.@감성` 형식입니다.\n",
    "여기서는 `Sentences_50Agree.txt` 파일을 기본으로 사용합니다. 다른 파일을 사용하려면 `file_path` 변수를 수정하세요.\n",
    "데이터셋은 [여기](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news/data) 등에서 다운로드 받을 수 있습니다.\n",
    "다운로드 후, 이 노트북과 같은 디렉토리에 해당 파일을 위치시키거나 `file_path`를 알맞게 수정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Sentences_50Agree.txt' # 기본 파일명, 필요시 Sentences_AllAgree.txt 등으로 변경\n",
    "encoding_to_try = ['utf-8', 'latin1', 'ISO-8859-1'] # 시도해볼 인코딩 목록\n",
    "\n",
    "df = None\n",
    "for encoding in encoding_to_try:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='.@', header=None, names=['text', 'sentiment'], engine='python', encoding=encoding)\n",
    "        print(f\"Successfully loaded data with encoding: {encoding}\")\n",
    "        break\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Please download it and place it in the correct directory.\")\n",
    "        df = None # 명시적으로 None 처리\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load with encoding {encoding}: {e}\")\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\nDataset loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nSentiment distribution:\")\n",
    "    print(df['sentiment'].value_counts())\n",
    "else:\n",
    "    print(\"\\nError: Could not load the dataset. Please check the file path and encoding.\")\n",
    "    # 데이터프레임이 로드되지 않았을 경우 이후 코드 실행을 막기 위해 예외 발생 또는 exit()\n",
    "    # 여기서는 간단히 비어있는 데이터프레임을 만들어서 이후 오류를 유도 (실제로는 더 강력한 처리 필요)\n",
    "    if df is None: df = pd.DataFrame(columns=['text', 'sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 데이터 전처리 및 라벨 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    # 라벨 인코딩 (positive: 0, neutral: 1, negative: 2)\n",
    "    # 주의: Hugging Face의 AutoModelForSequenceClassification은 일반적으로 레이블을 0부터 시작하는 정수로 기대합니다.\n",
    "    sentiment_to_id = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "    id_to_sentiment = {v: k for k, v in sentiment_to_id.items()}\n",
    "    num_fin_labels = len(sentiment_to_id)\n",
    "\n",
    "    df['label'] = df['sentiment'].map(sentiment_to_id)\n",
    "\n",
    "    # 누락된 라벨이 있는지 확인\n",
    "    if df['label'].isnull().any():\n",
    "        print(\"\\nWarning: Some sentiments were not mapped to labels. Check sentiment_to_id mapping and dataset values.\")\n",
    "        print(df[df['label'].isnull()])\n",
    "        df.dropna(subset=['label'], inplace=True) # NaN 라벨 제거\n",
    "        df['label'] = df['label'].astype(int) # 정수형으로 변환\n",
    "    \n",
    "    print(\"\\nData with encoded labels:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nNumber of labels: {num_fin_labels}\")\n",
    "    print(f\"Label mapping: {sentiment_to_id}\")\n",
    "else:\n",
    "    print(\"DataFrame is empty, skipping preprocessing.\")\n",
    "    num_fin_labels = 3 # 기본값, 실제로는 데이터 로드 성공 시 결정됨\n",
    "    sentiment_to_id = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "    id_to_sentiment = {v: k for k, v in sentiment_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 데이터 분할 (학습, 검증, 테스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'label' in df.columns:\n",
    "    # 학습/테스트 분할 (80% 학습, 20% 테스트)\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "    # 학습 데이터에서 검증 데이터 분할 (원래 학습 데이터의 12.5% -> 전체의 10%)\n",
    "    # train_test_split은 test_size 비율을 첫 번째 인자로 전달된 데이터프레임 기준으로 계산합니다.\n",
    "    # 따라서, train_df에서 0.125를 val_df로 할당하면, (0.8 * 0.125) = 0.1, 즉 전체의 10%가 됩니다.\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.125, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "    print(f\"Train set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "    print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "    # Hugging Face Dataset 형식으로 변환\n",
    "    train_dataset_hf = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    val_dataset_hf = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "    test_dataset_hf = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "    financial_datasets = DatasetDict({\n",
    "        'train': train_dataset_hf,\n",
    "        'validation': val_dataset_hf,\n",
    "        'test': test_dataset_hf\n",
    "    })\n",
    "    print(\"\\nFinancial Datasets (Hugging Face format):\")\n",
    "    print(financial_datasets)\n",
    "else:\n",
    "    print(\"DataFrame is empty or 'label' column is missing, skipping data splitting.\")\n",
    "    # 빈 DatasetDict 생성 (오류 방지용)\n",
    "    financial_datasets = DatasetDict({\n",
    "        'train': Dataset.from_dict({'text': [], 'label': []}),\n",
    "        'validation': Dataset.from_dict({'text': [], 'label': []}),\n",
    "        'test': Dataset.from_dict({'text': [], 'label': []})\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\" # GoEmotions와 동일 모델 사용 또는 변경 가능\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_financial(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128) # 금융 뉴스는 보통 짧음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and financial_datasets['train']:\n",
    "    tokenized_financial_datasets = financial_datasets.map(tokenize_function_financial, batched=True)\n",
    "    # 'text'와 'sentiment' 컬럼은 모델 학습에 직접 사용되지 않으므로 제거 (이미 'label'로 변환됨)\n",
    "    # 'Unnamed: 0', '__index_level_0__' 등 pandas에서 추가된 인덱스 컬럼도 제거\n",
    "    columns_to_remove = ['text', 'sentiment']\n",
    "    # 데이터셋에 따라 추가적으로 생성될 수 있는 인덱스 컬럼명 확인 및 추가\n",
    "    if 'Unnamed: 0' in tokenized_financial_datasets['train'].column_names:\n",
    "        columns_to_remove.append('Unnamed: 0')\n",
    "    if '__index_level_0__' in tokenized_financial_datasets['train'].column_names:\n",
    "        columns_to_remove.append('__index_level_0__')\n",
    "        \n",
    "    tokenized_financial_datasets = tokenized_financial_datasets.remove_columns(columns_to_remove)\n",
    "    tokenized_financial_datasets.set_format(\"torch\")\n",
    "\n",
    "    print(\"\\nTokenized Financial Datasets:\")\n",
    "    print(tokenized_financial_datasets)\n",
    "    if tokenized_financial_datasets['train']:\n",
    "      print(tokenized_financial_datasets['train'][0])\n",
    "else:\n",
    "    print(\"Dataset is empty, skipping tokenization.\")\n",
    "    # 빈 DatasetDict (오류 방지용)\n",
    "    tokenized_financial_datasets = DatasetDict({\n",
    "        'train': Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'label': []}),\n",
    "        'validation': Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'label': []}),\n",
    "        'test': Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'label': []})\n",
    "    })\n",
    "    tokenized_financial_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 선택 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_financial = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_fin_labels, \n",
    "    id2label=id_to_sentiment, \n",
    "    label2id=sentiment_to_id\n",
    ")\n",
    "\n",
    "# GPU 사용 가능 여부 확인 및 모델 이동\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_financial.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 학습 설정 및 메트릭 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_financial(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\", zero_division=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3 # 하루 안에 완료를 위해 에포크 수 조정\n",
    "\n",
    "training_args_financial = TrainingArguments(\n",
    "    output_dir=\"./results_financial\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_dir=\"./logs_financial\",\n",
    "    logging_steps=50, # 데이터셋 크기가 작을 수 있으므로 조정\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Trainer 정의 및 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_financial = Trainer(\n",
    "    model=model_financial,\n",
    "    args=training_args_financial,\n",
    "    train_dataset=tokenized_financial_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_financial_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_financial,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard 실행 (Colab 또는 로컬 터미널에서)\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs_financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and tokenized_financial_datasets['train']:\n",
    "    trainer_financial.train()\n",
    "else:\n",
    "    print(\"Skipping training as dataset is not loaded or processed correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 평가 및 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and tokenized_financial_datasets['test']:\n",
    "    eval_results_financial = trainer_financial.evaluate(tokenized_financial_datasets[\"test\"])\n",
    "    print(\"\\nTest Set Evaluation Results (Financial):\")\n",
    "    for key, value in eval_results_financial.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation as dataset is not loaded or processed correctly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and tokenized_financial_datasets['test']:\n",
    "    predictions_output_financial = trainer_financial.predict(tokenized_financial_datasets[\"test\"])\n",
    "    logits_financial = predictions_output_financial.predictions\n",
    "    true_labels_financial = predictions_output_financial.label_ids\n",
    "    \n",
    "    predicted_labels_financial = np.argmax(logits_financial, axis=-1)\n",
    "else:\n",
    "    print(\"Skipping prediction as dataset is not loaded or processed correctly.\")\n",
    "    # 빈 배열로 초기화하여 이후 시각화 코드에서 오류 방지\n",
    "    true_labels_financial = np.array([])\n",
    "    predicted_labels_financial = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 분류 보고서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if true_labels_financial.size > 0:\n",
    "    financial_label_names = [id_to_sentiment[i] for i in range(num_fin_labels)]\n",
    "    print(\"\\nFinancial PhraseBank Classification Report (Test Set):\")\n",
    "    print(classification_report(true_labels_financial, predicted_labels_financial, target_names=financial_label_names, zero_division=0))\n",
    "else:\n",
    "    print(\"Cannot generate classification report: No predictions available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 혼동 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if true_labels_financial.size > 0:\n",
    "    cm_financial = confusion_matrix(true_labels_financial, predicted_labels_financial)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_financial, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=financial_label_names, yticklabels=financial_label_names)\n",
    "    plt.title(\"Financial PhraseBank Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot generate confusion matrix: No predictions available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 샘플 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_financial_sentiment(text, model, tokenizer, id2label):\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    predicted_class_id = logits.argmax().item()\n",
    "    predicted_label = id2label[predicted_class_id]\n",
    "    \n",
    "    return predicted_label, torch.softmax(logits, dim=1).squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'label' in df.columns: # 모델 학습이 시도되었는지 확인\n",
    "    sample_financial_texts = [\n",
    "        \"The company reported strong earnings growth this quarter.\",\n",
    "        \"Market sentiment remains neutral amidst global uncertainties.\",\n",
    "        \"Analysts predict a downturn in stock prices next week.\",\n",
    "        \"Despite the volatile market, our portfolio showed a slight increase.\",\n",
    "        \"The new regulations are expected to negatively impact the industry.\"\n",
    "    ]\n",
    "\n",
    "    for text in sample_financial_texts:\n",
    "        # 학습된 model_financial 사용\n",
    "        predicted_sentiment, probs = predict_financial_sentiment(text, model_financial, tokenizer, id_to_sentiment)\n",
    "        print(f\"\\nSample Financial Text: '{text}'\")\n",
    "        print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "        # print(f\"Probabilities: {probs}\") # 각 클래스에 대한 확률 출력 (선택 사항)\n",
    "else:\n",
    "    print(\"\\nSkipping sample prediction as model was not trained due to data loading/processing issues.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 저장 (선택 사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and tokenized_financial_datasets['train'] and hasattr(trainer_financial, 'model'):\n",
    "    output_model_dir_financial = \"./saved_model_financial\"\n",
    "    os.makedirs(output_model_dir_financial, exist_ok=True)\n",
    "\n",
    "    trainer_financial.save_model(output_model_dir_financial)\n",
    "    tokenizer.save_pretrained(output_model_dir_financial)\n",
    "\n",
    "    print(f\"Financial model and tokenizer saved to {output_model_dir_financial}\")\n",
    "else:\n",
    "    print(\"Skipping model saving as model was not trained or does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드 및 사용 예시 (저장된 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # 저장된 모델과 토크나이저 로드\n",
    "# if os.path.exists(output_model_dir_financial if 'output_model_dir_financial' in locals() else './saved_model_financial_placeholder'):\n",
    "#     loaded_model_financial = AutoModelForSequenceClassification.from_pretrained(output_model_dir_financial)\n",
    "#     loaded_tokenizer_financial = AutoTokenizer.from_pretrained(output_model_dir_financial)\n",
    "\n",
    "#     # id_to_sentiment 맵핑 (실제 사용 시 저장/로드 필요)\n",
    "#     # 이 예제에서는 위에서 정의된 id_to_sentiment를 사용합니다.\n",
    "\n",
    "#     sample_text_for_loading_test = \"The acquisition is expected to boost profits significantly.\"\n",
    "#     predicted_sentiment_loaded, _ = predict_financial_sentiment(sample_text_for_loading_test, loaded_model_financial, loaded_tokenizer_financial, id_to_sentiment)\n",
    "\n",
    "#     print(f\"\\nSample Financial Text (loaded model): '{sample_text_for_loading_test}'\")\n",
    "#     print(f\"Predicted Sentiment: {predicted_sentiment_loaded}\")\n",
    "# else:\n",
    "#     print(\"Saved model directory not found. Skipping loading example.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
