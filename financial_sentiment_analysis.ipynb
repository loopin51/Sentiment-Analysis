{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial PhraseBank: 금융 감성 분석 모델 학습\n",
    "\n",
    "이 노트북은 Financial PhraseBank 데이터셋을 사용하여 금융 뉴스 헤드라인의 감성(긍정/중립/부정)을 분류하는 모델을 학습하고 평가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers[torch] scikit-learn pandas numpy matplotlib seaborn tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Hub 로그인 (필요시)\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 준비\n",
    "\n",
    "Financial PhraseBank 데이터셋은 일반적으로 `.txt` 파일 형태로 제공되며, 각 라인은 `텍스트.@감성` 형식입니다.\n",
    "여기서는 `Sentences_50Agree.txt` 파일을 기본으로 사용합니다. 다른 파일을 사용하려면 `file_path` 변수를 수정하세요.\n",
    "데이터셋은 [여기](https://www.kaggle.com/datasets/ankurzing/sentiment-analysis-for-financial-news/data) 등에서 다운로드 받을 수 있습니다.\n",
    "다운로드 후, 이 노트북과 같은 디렉토리에 해당 파일을 위치시키거나 `file_path`를 알맞게 수정해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Sentences_50Agree.txt' \n",
    "encoding_to_try = ['utf-8', 'latin1', 'ISO-8859-1'] \n",
    "\n",
    "df = None\n",
    "for encoding in encoding_to_try:\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='.@', header=None, names=['text', 'sentiment'], engine='python', encoding=encoding)\n",
    "        print(f\"Successfully loaded data with encoding: {encoding}\")\n",
    "        break\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Please download it and place it in the correct directory.\")\n",
    "        df = None \n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load with encoding {encoding}: {e}\")\n",
    "\n",
    "if df is not None and not df.empty:\n",
    "    print(\"\\nDataset loaded successfully.\")\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    print(df.head())\n",
    "else:\n",
    "    print(\"\\nError: Could not load the dataset. Please check the file path and encoding.\")\n",
    "    if df is None: df = pd.DataFrame(columns=['text', 'sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. 데이터셋 레이블 분포 시각화 (추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df is not None and not df.empty and 'sentiment' in df.columns:\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if not sentiment_counts.empty:\n",
    "        if len(sentiment_counts) > 3 : # 많은 카테고리면 막대 그래프\n",
    "             sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette='viridis')\n",
    "             plt.title('Financial PhraseBank: Sentiment Distribution')\n",
    "             plt.xlabel('Sentiment')\n",
    "             plt.ylabel('Number of Sentences')\n",
    "        else: # 적은 카테고리면 파이 차트\n",
    "            plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('viridis', len(sentiment_counts)))\n",
    "            plt.title('Financial PhraseBank: Sentiment Distribution')\n",
    "            plt.axis('equal') # 파이차트를 원형으로 만듭니다.\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Sentiment column is empty or does not exist for visualization.\")\n",
    "else:\n",
    "    print(\"DataFrame is empty or 'sentiment' column is missing, skipping sentiment distribution visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 데이터 전처리 및 라벨 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    sentiment_to_id = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "    id_to_sentiment = {v: k for k, v in sentiment_to_id.items()}\n",
    "    num_fin_labels = len(sentiment_to_id)\n",
    "\n",
    "    df['label'] = df['sentiment'].map(sentiment_to_id)\n",
    "\n",
    "    if df['label'].isnull().any():\n",
    "        print(\"\\nWarning: Some sentiments were not mapped to labels. Check sentiment_to_id mapping and dataset values.\")\n",
    "        print(df[df['label'].isnull()])\n",
    "        df.dropna(subset=['label'], inplace=True) \n",
    "        df['label'] = df['label'].astype(int) \n",
    "    \n",
    "    print(\"\\nData with encoded labels:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nNumber of labels: {num_fin_labels}\")\n",
    "    print(f\"Label mapping: {sentiment_to_id}\")\n",
    "else:\n",
    "    print(\"DataFrame is empty, skipping preprocessing.\")\n",
    "    num_fin_labels = 3 \n",
    "    sentiment_to_id = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "    id_to_sentiment = {v: k for k, v in sentiment_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. 데이터 분할 (학습, 검증, 테스트)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'label' in df.columns:\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.125, random_state=42, stratify=train_df['label'])\n",
    "\n",
    "    print(f\"Train set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "    print(f\"Test set size: {len(test_df)}\")\n",
    "\n",
    "    train_dataset_hf = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "    val_dataset_hf = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "    test_dataset_hf = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "    financial_datasets = DatasetDict({\n",
    "        'train': train_dataset_hf,\n",
    "        'validation': val_dataset_hf,\n",
    "        'test': test_dataset_hf\n",
    "    })\n",
    "    print(\"\\nFinancial Datasets (Hugging Face format):\")\n",
    "    print(financial_datasets)\n",
    "else:\n",
    "    print(\"DataFrame is empty or 'label' column is missing, skipping data splitting.\")\n",
    "    financial_datasets = DatasetDict({\n",
    "        'train': Dataset.from_dict({'text': [], 'label': []}),\n",
    "        'validation': Dataset.from_dict({'text': [], 'label': []}),\n",
    "        'test': Dataset.from_dict({'text': [], 'label': []})\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"distilbert-base-uncased\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_financial(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and financial_datasets['train']:\n",
    "    tokenized_financial_datasets = financial_datasets.map(tokenize_function_financial, batched=True)\n",
    "    columns_to_remove = ['text', 'sentiment']\n",
    "    if 'Unnamed: 0' in tokenized_financial_datasets['train'].column_names:\n",
    "        columns_to_remove.append('Unnamed: 0')\n",
    "    if '__index_level_0__' in tokenized_financial_datasets['train'].column_names:\n",
    "        columns_to_remove.append('__index_level_0__')\n",
    "        \n",
    "    # remove_columns_ 사용 시 오류 발생 가능성 있어 remove_columns로 변경\n",
    "    for split in tokenized_financial_datasets.keys():\n",
    "        # 현재 split에 존재하는 컬럼만 제거 시도\n",
    "        actual_columns_to_remove = [col for col in columns_to_remove if col in tokenized_financial_datasets[split].column_names]\n",
    "        tokenized_financial_datasets[split] = tokenized_financial_datasets[split].remove_columns(actual_columns_to_remove)\n",
    "    tokenized_financial_datasets.set_format(\"torch\")\n",
    "\n",
    "    print(\"\\nTokenized Financial Datasets:\")\n",
    "    print(tokenized_financial_datasets)\n",
    "    if tokenized_financial_datasets['train']:\n",
    "      print(tokenized_financial_datasets['train'][0])\n",
    "else:\n",
    "    print(\"Dataset is empty, skipping tokenization.\")\n",
    "    tokenized_financial_datasets = DatasetDict({\n",
    "        'train': Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'label': []}),\n",
    "        'validation': Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'label': []}),\n",
    "        'test': Dataset.from_dict({'input_ids': [], 'attention_mask': [], 'label': []})\n",
    "    })\n",
    "    tokenized_financial_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 모델 선택 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_financial = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    num_labels=num_fin_labels, \n",
    "    id2label=id_to_sentiment, \n",
    "    label2id=sentiment_to_id\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_financial.to(device)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. 학습 설정 및 메트릭 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_financial(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1_macro\": f1_score(labels, predictions, average=\"macro\", zero_division=0)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "training_args_financial = TrainingArguments(\n",
    "    output_dir=\"./results_financial\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_dir=\"./logs_financial\",\n",
    "    logging_strategy=\"epoch\", # 로깅 전략을 epoch으로 변경\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Trainer 정의 및 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_financial = Trainer(\n",
    "    model=model_financial,\n",
    "    args=training_args_financial,\n",
    "    train_dataset=tokenized_financial_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_financial_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_financial,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard 실행 (Colab 또는 로컬 터미널에서)\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir logs_financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and tokenized_financial_datasets['train'] and len(tokenized_financial_datasets['train']) > 0:\n",
    "    train_result_financial = trainer_financial.train()\n",
    "else:\n",
    "    print(\"Skipping training as dataset is not loaded or processed correctly, or is empty.\")\n",
    "    train_result_financial = None # 학습이 스킵되었음을 명시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. 학습 과정 시각화 (추가)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_result_financial is not None: # 학습이 진행되었을 경우에만 시각화\n",
    "    log_history_financial = trainer_financial.state.log_history\n",
    "    df_log_financial = pd.DataFrame(log_history_financial)\n",
    "\n",
    "    train_loss_fin = df_log_financial[df_log_financial['loss'].notna()][['epoch', 'loss']]\n",
    "    eval_loss_fin = df_log_financial[df_log_financial['eval_loss'].notna()][['epoch', 'eval_loss']]\n",
    "    eval_metrics_fin = df_log_financial[df_log_financial['eval_f1_macro'].notna()]\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if not train_loss_fin.empty:\n",
    "        plt.plot(train_loss_fin['epoch'], train_loss_fin['loss'], label='Training Loss', marker='o')\n",
    "    if not eval_loss_fin.empty:\n",
    "        plt.plot(eval_loss_fin['epoch'], eval_loss_fin['eval_loss'], label='Validation Loss', marker='o')\n",
    "    plt.title('Financial Model: Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    if not eval_metrics_fin.empty:\n",
    "        plt.plot(eval_metrics_fin['epoch'], eval_metrics_fin['eval_f1_macro'], label='Validation F1 Macro', marker='o', color='green')\n",
    "        if 'eval_accuracy' in eval_metrics_fin.columns:\n",
    "            plt.plot(eval_metrics_fin['epoch'], eval_metrics_fin['eval_accuracy'], label='Validation Accuracy', marker='x', color='purple', linestyle='--')\n",
    "    plt.title('Financial Model: Validation Metrics')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping training visualization as training was not performed or log history is unavailable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 모델 평가 및 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and tokenized_financial_datasets['test'] and len(tokenized_financial_datasets['test']) > 0 and train_result_financial is not None:\n",
    "    eval_results_financial = trainer_financial.evaluate(tokenized_financial_datasets[\"test\"])\n",
    "    print(\"\\nTest Set Evaluation Results (Financial):\")\n",
    "    for key, value in eval_results_financial.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping evaluation as dataset is not loaded, processed correctly, is empty, or model was not trained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels_financial = np.array([])\n",
    "predicted_labels_financial = np.array([])\n",
    "\n",
    "if not df.empty and tokenized_financial_datasets['test'] and len(tokenized_financial_datasets['test']) > 0 and train_result_financial is not None:\n",
    "    predictions_output_financial = trainer_financial.predict(tokenized_financial_datasets[\"test\"])\n",
    "    logits_financial = predictions_output_financial.predictions\n",
    "    true_labels_financial = predictions_output_financial.label_ids\n",
    "    predicted_labels_financial = np.argmax(logits_financial, axis=-1)\n",
    "else:\n",
    "    print(\"Skipping prediction as dataset is not loaded, processed correctly, is empty, or model was not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. 분류 보고서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if true_labels_financial.size > 0:\n",
    "    financial_label_names = [id_to_sentiment[i] for i in sorted(id_to_sentiment.keys())]\n",
    "    print(\"\\nFinancial PhraseBank Classification Report (Test Set):\")\n",
    "    print(classification_report(true_labels_financial, predicted_labels_financial, target_names=financial_label_names, zero_division=0))\n",
    "else:\n",
    "    print(\"Cannot generate classification report: No predictions available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. 혼동 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if true_labels_financial.size > 0:\n",
    "    cm_financial = confusion_matrix(true_labels_financial, predicted_labels_financial)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm_financial, annot=True, fmt=\"d\", cmap=\"Blues\", \n",
    "                xticklabels=financial_label_names, yticklabels=financial_label_names)\n",
    "    plt.title(\"Financial PhraseBank Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot generate confusion matrix: No predictions available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 샘플 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_financial_sentiment(text, model_to_use, tokenizer_to_use, current_id2label):\n",
    "    model_to_use.eval()\n",
    "    current_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_to_use.to(current_device)\n",
    "\n",
    "    inputs = tokenizer_to_use(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    inputs = {k: v.to(current_device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_output = model_to_use(**inputs).logits\n",
    "    \n",
    "    predicted_class_id = logits_output.argmax().item()\n",
    "    predicted_label_name = current_id2label[predicted_class_id]\n",
    "    \n",
    "    return predicted_label_name, torch.softmax(logits_output, dim=1).squeeze().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and 'label' in df.columns and train_result_financial is not None:\n",
    "    sample_financial_texts = [\n",
    "        \"The company reported strong earnings growth this quarter.\",\n",
    "        \"Market sentiment remains neutral amidst global uncertainties.\",\n",
    "        \"Analysts predict a downturn in stock prices next week.\",\n",
    "        \"Despite the volatile market, our portfolio showed a slight increase.\",\n",
    "        \"The new regulations are expected to negatively impact the industry.\"\n",
    "    ]\n",
    "\n",
    "    for text_item in sample_financial_texts:\n",
    "        predicted_sentiment, probs = predict_financial_sentiment(text_item, model_financial, tokenizer, id_to_sentiment)\n",
    "        print(f\"\\nSample Financial Text: '{text_item}'\")\n",
    "        print(f\"Predicted Sentiment: {predicted_sentiment}\")\n",
    "else:\n",
    "    print(\"\\nSkipping sample prediction as model was not trained or data was not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 모델 저장 (선택 사항)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty and tokenized_financial_datasets['train'] and hasattr(trainer_financial, 'model') and train_result_financial is not None:\n",
    "    output_model_dir_financial = \"./saved_model_financial\"\n",
    "    os.makedirs(output_model_dir_financial, exist_ok=True)\n",
    "\n",
    "    trainer_financial.save_model(output_model_dir_financial)\n",
    "    tokenizer.save_pretrained(output_model_dir_financial)\n",
    "\n",
    "    print(f\"Financial model and tokenizer saved to {output_model_dir_financial}\")\n",
    "else:\n",
    "    print(\"Skipping model saving as model was not trained, does not exist, or data was not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 로드 및 사용 예시 (저장된 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir_financial_path = \"./saved_model_financial\" # 변수명 일관성 유지\n",
    "if os.path.exists(output_model_dir_financial_path) and os.path.exists(os.path.join(output_model_dir_financial_path, 'pytorch_model.bin')):\n",
    "    loaded_model_financial = AutoModelForSequenceClassification.from_pretrained(output_model_dir_financial_path)\n",
    "    loaded_tokenizer_financial = AutoTokenizer.from_pretrained(output_model_dir_financial_path)\n",
    "\n",
    "    sample_text_for_loading_test = \"The acquisition is expected to boost profits significantly.\"\n",
    "    predicted_sentiment_loaded, _ = predict_financial_sentiment(sample_text_for_loading_test, loaded_model_financial, loaded_tokenizer_financial, id_to_sentiment)\n",
    "\n",
    "    print(f\"\\nSample Financial Text (loaded model): '{sample_text_for_loading_test}'\")\n",
    "    print(f\"Predicted Sentiment: {predicted_sentiment_loaded}\")\n",
    "else:\n",
    "    print(f\"Skipping loading example: Model not found at {output_model_dir_financial_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
